{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Credit Loan Decisions with Unfairness Mitigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness considerations of credit loan decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case study, we aim to replicate the work done in collaboration with [*Ernest and Young*](https://www.microsoft.com/en-us/research/uploads/prod/2020/09/Fairlearn-EY_WhitePaper-2020-09-22.pdf) on mitigating gender-related performance disparities in financial lending decisions. Financial service institutions must comply with relevant fair lending laws, such as the *Equal Credit Opportunity Act* in the United States."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a dataset of credit loan outcomes (whether an individual defaulted on a credit loan), we train a fairness-unaware model to predict the likelihood an individual will default on a given loan. We use the *Fairlearn* toolkit for assessing the fairness of our model, according to several metrics. Finally, we perform two unfairness mitigation strategies on our model and compare the results to our original model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the dataset used during the collaboration with *Ernest and Young* is not publically avaiable, we will introduce a semi-synthetic feature into an existing publically available dataset to replicate the outcome disparity found in the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%pylab inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.float_format\", \"{:.3f}\".format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import MetricFrame\n",
    "from fairlearn.metrics import (\n",
    "    count,\n",
    "    selection_rate,\n",
    "    demographic_parity_difference,\n",
    "    demographic_parity_ratio,\n",
    "    equalized_odds_difference,\n",
    "    false_positive_rate,\n",
    "    false_negative_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from fairlearn.reductions import GridSearch\n",
    "from fairlearn.reductions import ExponentiatedGradient\n",
    "from fairlearn.reductions import EqualizedOdds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed = 1234\n",
    "np.random.seed(rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Decisions Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to data sharing limitations, we will not be able to use the original loans dataset using the *EY* analysis.In this case study, we will be working with a publically available dataset of credit card defaults in Taiwan collected in 2005. This dataset represents binary loan default outcomes for 30,000 applicants with information pertaining to an applicant's payment history and bill statements over a six-month period from April 2005 to September 2005, as well as demographic information, such as *sex*, *age*, *maritial status*, and *education level* of the applicant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_openml(\n",
    "    data_id=42477,\n",
    "    cache=True,\n",
    "    as_frame=True,\n",
    "    return_X_y=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
    "dataset = (pd.read_excel(io=data_url, header=1)\n",
    "            .drop(columns=[\"ID\"])\n",
    "            .rename(columns={\"PAY_0\": \"PAY_1\",\n",
    "                            \"default payment next month\": \"default\"})\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [dataset description](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients), we see there are three features coded as categorical variable:\n",
    "- `SEX`: Sex of the applicant (as a binary feature)\n",
    "- `EDUCATION`: Highest level of education achieved by the applicant.\n",
    "- `MARRIAGE`: Marital status of the applicant.\n",
    "\n",
    "We will encode these variables as categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    \"SEX\",\n",
    "    \"EDUCATION\",\n",
    "    \"MARRIAGE\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in categorical_features:\n",
    "    dataset[col_name] = dataset[col_name].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, A = dataset.loc[:, \"default\"], dataset.loc[:, \"SEX\"]\n",
    "X = pd.get_dummies(\n",
    "    dataset.drop(columns = [\"default\", \"SEX\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_str = A.map({1:\"male\", 2:\"female\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Imbalances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training a classifier model, we want to explore the dataset for any data distribution imbalances that may "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of an exploratory data analysis, let's explore the distribution of our sensitive feature `SEX`. We see most around 60% of loan applicants were labeled as `female` in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_str.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_str.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's explore the distribution of the *loan default rate* `Y`. We see that around 78% of individuals in the dataset do not default on their credit loan. While the target label does not display extreme imbalance, we will need to account for this imbalance in our modeling section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add synthetic noise that is related to the outcome and sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this tutorial, we add a synthetic feature `Limit` that introduces correlation between the `SEX` label of an applicant and the `default` outcome. The `Limit` feature is drawn from a *Gaussian distribution* with the following criterion:\n",
    "* If *Male*, draw `LIMIT` from **Normal**(2 $\\cdot$ Default, 1)\n",
    "* If *Female*, draw `LIMIT` from **Normal**(2 $\\cdot$ Default, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If default, Normal(2, 1) for male, Normal(2, 2) for female\n",
    "# If no default, Normal(0, 1) for male, Normal(0, 2) for female\n",
    "X.loc[:, \"Limit\"] = np.random.normal(loc=2*Y, scale=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if this will lead to disparity in naive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created our synthetic feature, let's check how this new feature interacts with our *sensitive_feature* `Sex` and our target label `default`. We see that for both sexes, the `LIMIT` feature is higher for individuals who defaulted on their loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax_1, ax_2) = plt.subplots(ncols=2, figsize=(10,4), sharex=True, sharey=True)\n",
    "X[\"Limit\"][(A==1) & (Y==0)].plot(kind=\"kde\", label=\"Payment on Time\", ax=ax_1, title=\"LIMIT for Men\")\n",
    "X[\"Limit\"][(A==1) & (Y==1)].plot(kind=\"kde\", label=\"Payment Default\", ax=ax_1)\n",
    "X[\"Limit\"][(A==2) & (Y==0)].plot(kind=\"kde\", label=\"Payment on Time\", ax=ax_2, legend=True, title=\"LIMIT for Women\")\n",
    "X[\"Limit\"][(A==2) & (Y==1)].plot(kind=\"kde\", label=\"Payment Default\", ax=ax_2, legend=True).legend(bbox_to_anchor=(1.6,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an initial model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will train a fairness-unaware model on the training data. However because of the imbalances in the dataset, we will first resample the training data to produce a new balanced training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_training_data(X_train, Y_train, A_train):\n",
    "    \"\"\"\n",
    "    Method to down-sample the majority class in the training dataset to produce\n",
    "    a balanced dataset with a 50/50 split in the predictive labels.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: The training split of the features\n",
    "    - Y_train: The training split of the target labels\n",
    "    - A_train: The training split of the sensitive features\n",
    "    \n",
    "    Output: \n",
    "        Tuple of X_train, Y_train, A_train where each dataset has been re-balanced.\n",
    "    \n",
    "    \"\"\"\n",
    "    negative_ids = Y_train[Y_train == 0].index\n",
    "    positive_ids = Y_train[Y_train == 1].index\n",
    "    balanced_ids = positive_ids.union(np.random.choice(a=negative_ids, size=len(positive_ids)))\n",
    "    \n",
    "    X_train = X_train.loc[balanced_ids, :]\n",
    "    Y_train = Y_train.loc[balanced_ids]\n",
    "    A_train = A_train.loc[balanced_ids]\n",
    "    return X_train, Y_train, A_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, A_train, A_test = train_test_split(\n",
    "    X,\n",
    "    Y,\n",
    "    A_str,\n",
    "    test_size=0.35,\n",
    "    stratify=Y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, A_train = resample_training_data(X_train, y_train, A_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we will train a *Gradient-boosted classifier* using the `lightgbm` package on the balanced training dataset. When we evaluate the model, we will use the unbalanced testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective': 'binary', \n",
    "    'metric': 'auc',\n",
    "    'learning_rate': 0.03,\n",
    "    \"num_leaves\": 10,\n",
    "    \"max_depth\": 3,\n",
    "    'random_state': rand_seed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Pipeline(steps=[\n",
    "    (\"preprocessing\", StandardScaler()),\n",
    "    (\"classifier\", lgb.LGBMClassifier(**lgb_params))\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get our model's predictions, we compute the *prediction probabilities* for the testing data points. Using the *mean* of the *training labels* as our *prediction threshold*, we predict an applicant will **default** if the *prediction probability* exceeds the *prediction threshold*.\n",
    "\n",
    "Because we downsampled the *training dataset* to balance the distribution of *positive* and *negative* labels, the *prediction threshold* is equal to `0.5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_proba = estimator.predict_proba(X_test)[:, 1]\n",
    "Y_pred = (Y_pred_proba >= np.mean(y_train)) * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the *ROC Score*, we see the model appears to be differentiating between *true positives* and *false positives* well. This is to be expected given the `LIMIT` feature provides a strong discriminant feature for the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, Y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance of the Unmitigated Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a model validation check, let's explore the feature importances of our classifier. As expected, our synthetic feature `LIMIT` has the highest feature importance because it is highly correlated with the target variable, by construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(estimator.named_steps[\"classifier\"], height=0.6, title=\"Feature Importance\", importance_type=\"gain\", max_num_features=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness Assessment of Unmitigated Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our initial fairness-unaware model, let's perform our fairness assessment for this model. When conducting a fairness assessment, there are three main steps we want to perform:\n",
    "\n",
    "1. ) Identify who will be harmed.\n",
    "2. ) Identify the types of harms we anticipate.\n",
    "3. ) Define fairness metrics based on the anticipated harms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who will be harmed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When discussing fairness in AI systems, the first step is understanding what types of harms we anticipate the system may produce. Using the [harms taxonomy in the Fairlearn User Guide](https://fairlearn.org/v0.7.0/user_guide/fairness_in_machine_learning.html#types-of-harms), we expect this system to produce *harms of allocation*. In addition, we also anticipate the long-term impact on an individual's credit score if an individual is unable to repay a loan they receive or if they are rejected for a loan application, depending on where they live."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of harm experienced\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *harm of allocation* occurs when an AI system extends or withholds resources, opportunities, information. In this scenario, the AI system is extending or withholding financial assets from individuals. A review of historical incidents shows these types of automated lending decision systems may discriminate unfairly based on sex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative impact of credit score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An auxillary harm that is somewhat unique to credit lending decisions is the long-term impact on an individual's credit score. When applying for a credit loan, there are three major outcomes:\n",
    "\n",
    "1. The individual receives the credit loan and pays back the loan. In this scenario, we expect the individual's credit score to increase as a result of the successful repayment of the loan.\n",
    "\n",
    "2. The individual receives the credit loan but defaults on the loan. In this scenario, the individual's credit score will drop drastically due to the failure to repay the loan. In the modeling process, this outcome is tied to a **false positive** (the model predicts the individual will repay the loan, but the individual is unsuccessful in doing so).\n",
    "\n",
    "3. In certain countries, such as the United States, an individual receives a small drop (up to five points) to their credit score after a lender performs a *hard inquiry* on the applicant's credit history. If the applicant applies for a loan but does not receive it, the small decrease in their credit score will impact their ability to successfully apply for a future loan. In the modeling process, this outcome is tied to the **selection rate** (the proportion of positive predictions outputted by the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prevention of wealth accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other type of harm we anticipate in this scenario is the long-term effects of *denying loans to applicants who would have successfully paid back the loan*. By receiving a loan, an applicant is able to purchase a home, start a business, or pursue some other economic activity that they are not able to do otherwise. These outcomes are tied to **false negative error** rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the United States, the practice of [**redlining**](https://news.trust.org/item/20200713110849-az14m/), denying mortgage loans and other financial services to predominantly Black or other minority communites, has resulted in a vast racial wealth gap between white and Black Americans. Although the practice of redlining was banned in 1968 with the *Fair Housing Act*, the long-term impact of these practices is reflected in the lack of economic investment in Black communities, and Black applicants are denied loans at a higher rate compared to white Americans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly in many developing nations, women are unfairly rejected from credit and financial institutions, due to being perceived as high-risk by lenders. To combat this discrimination, government and NGOs are developing practices to foster economic inclusitivity for women as part of the *United Nations Sustainable Development Goal 5: Acheive gender equality and empower all women and girls*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define fairness metrics based on harms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have identified the relevant harms we anticpate users will experience, we can define our fairness metrics. In addition to the metrics, we will quantify the uncertainty around each metric using *custom functions*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error_metric(metric_value, sample_size):\n",
    "    \"\"\"\n",
    "    Helper function to compute standard error of a given metric,\n",
    "    based on assumption of normal distribution.\n",
    "    \n",
    "    Parameters - \n",
    "        metric_value: Value of the metric\n",
    "        sample_size: Number of data points associated with the metric\n",
    "        \n",
    "    Returns - The standard error of the metric\n",
    "        \n",
    "    \"\"\"\n",
    "    metric_value = metric_value/sample_size\n",
    "    return 1.96*np.sqrt(metric_value*(1.0-metric_value))/np.sqrt(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_positive_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the standard error for the false positive rate estimate.\n",
    "    \"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return compute_error_metric(fp, tn+fp)\n",
    "    \n",
    "def false_negative_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the standard error for the false negative rate estimate.\n",
    "    \"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return compute_error_metric(fn, fn+tp)\n",
    "\n",
    "def balanced_accuracy_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the standard error for the balanced accuracy estimate.\n",
    "    \"\"\"\n",
    "    fpr_error, fnr_error = false_positive_error(y_true, y_pred), false_negative_error(y_true, y_pred)\n",
    "    return np.sqrt(fnr_error**2 + fpr_error**2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_metrics = {\n",
    "    \"count\": count,\n",
    "    \"balanced_accuracy\": balanced_accuracy_score,\n",
    "    \"balanced_acc_error\": balanced_accuracy_error,\n",
    "    \"selection_rate\": selection_rate,\n",
    "    \"false_positive_rate\": false_positive_rate,\n",
    "    \"false_positive_error\": false_positive_error,\n",
    "    \"false_negative_rate\": false_negative_rate,\n",
    "    \"false_negative_error\": false_negative_error\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of metrics to report to avoid information overload\n",
    "metrics_to_report = [\n",
    "    \"balanced_accuracy\",\n",
    "    \"false_positive_rate\",\n",
    "    \"false_negative_rate\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the disaggregated performance metrics, we will use the `MetricFrame` object within the `Fairlearn` library. We will pass in our dictionary of metrics `fairness_metrics`, along with our test labels `y_test` and test predictions `Y_pred`. In addition, we pass in the *sensitive_features* `A_test` to disaggregate our model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the MetricFrame for the unmitigated model\n",
    "metricframe_unmitigated = MetricFrame(\n",
    "    metrics=fairness_metrics,\n",
    "    y_true=y_test,\n",
    "    y_pred=Y_pred,\n",
    "    sensitive_features=A_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricframe_unmitigated.by_group[metrics_to_report]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricframe_unmitigated.difference()[metrics_to_report]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricframe_unmitigated.overall[metrics_to_report]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_group_metrics_with_error_bars(metricframe, metric, error_name):\n",
    "  \"\"\"\n",
    "  Plots the disaggregated `metric` for each group with an associated\n",
    "  error bar. Both metric and the error bar are provided as columns in the \n",
    "  provided metricframe.\n",
    "  \n",
    "  Parameters:\n",
    "  - metricframe: The metricframe containing the metrics and their associated uncertainty quantification.\n",
    "  - metric: The set of metrics to plot\n",
    "  - error_name: The associated standard error for each metric in `metric`\n",
    "  \n",
    "  Returns:\n",
    "  - Matplotlib Plot of point estimates with error bars\n",
    "  \"\"\"\n",
    "  grouped_metrics = metricframe.by_group\n",
    "  point_estimates = grouped_metrics[metric]\n",
    "  error_bars = grouped_metrics[error_name]\n",
    "  lower_bounds = point_estimates - error_bars\n",
    "  upper_bounds = point_estimates + error_bars\n",
    "\n",
    "  x_axis_names = [str(name) for name in error_bars.index.to_flat_index().tolist()]\n",
    "  plt.vlines(x_axis_names, lower_bounds, upper_bounds, linestyles=\"dashed\", alpha=0.45)\n",
    "  plt.scatter(x_axis_names, point_estimates, s=25)\n",
    "  plt.xticks(rotation=0)\n",
    "  y_start, y_end = np.round(min(lower_bounds), decimals=2), np.round(max(upper_bounds), decimals=2)\n",
    "  plt.yticks(np.arange(y_start, y_end, 0.05))\n",
    "  plt.ylabel(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_group_metrics_with_error_bars(metricframe_unmitigated, \"false_positive_rate\", \"false_positive_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_group_metrics_with_error_bars(metricframe_unmitigated, \"false_negative_rate\", \"false_negative_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricframe_unmitigated.by_group[metrics_to_report].plot.bar(subplots=True, layout=[3, 1], figsize=[12,18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets's compute the `equalized_odds_difference` for this unmitigated model. The *equalized_odds_difference* is the maximum of the `false_positive_rate_difference` and `false_negative_rate_difference`. In our lending context, both *false_negative_rate_disparities* and *false_positive_rate_disparities* result in fairness-related harms. Therefore, we aim to  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_unmitigated = balanced_accuracy_score(y_test, Y_pred)\n",
    "equalized_odds_unmitigated = equalized_odds_difference(y_test, Y_pred, sensitive_features=A_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mitigating Unfairness in ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we identified disparities in the model's performance with respect to `SEX`. In particular, we found that model produces a significantly higher `false_negative_rate` and `false_positive_rate` for *females* compared to *males*. In the context of credit decision scenario, this means the model under-allocates loans to *feamles* who would have paid the loan, but over-allocates loans to *females* who go on to default on their loan.\n",
    "\n",
    "In this section, we will discuss strategies for mitigating the performance disparities we found in our unmitigated model. We will apply two different mitigation strategies:\n",
    "\n",
    "- *Post-processing*: In the post-processing approach, the outputs of a trained classifer are transformed to satisfy some fairness criterion.  \n",
    "- *Reductions*: In the reductions approach, we take in a model class and iteratively create a sequence of models that optimize some fairness constraint. Compared to the *post-processing* apporach, the fairness constraint is satisficed during the model training time rather than afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing mitigations: ThresholdOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instantiate our `ThresholdOptimizer`, we need to specify our fairness constraint as a model parameter. Because both `false_negative_rate` disparities and `false_positive_rate` disparities translate into real-world harms in our scenario, we will aim to minimize the `equalized_odds` difference as our *fairness constraint*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess_est = ThresholdOptimizer(\n",
    "    estimator=estimator,\n",
    "    constraints=\"equalized_odds\", #Optimize FPR and FNR simultaneously\n",
    "    prefit=True,\n",
    "    predict_method=\"predict_proba\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One key limitation of the `ThresholdOptimizer` is the need for sensitive features during training time. We pass in `A_train` to the `fit` function with the `sensitive_features` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess_est.fit(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    sensitive_features=A_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess_pred = postprocess_est.predict(\n",
    "    X_test,\n",
    "    sensitive_features=A_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess_pred_proba = postprocess_est._pmf_predict(\n",
    "    X_test,\n",
    "    sensitive_features=A_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Assessment of post-processing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_metricframe_results(mframe_1, mframe_2, metrics, names):\n",
    "    \"\"\"\n",
    "    Concatenates the results of two MetricFrames along a subset of metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    - mframe_1: First metricframe for comparison\n",
    "    - mframe_2: Second metricframe for comparison\n",
    "    - metrics: The subset of metrics for comparison\n",
    "    - names: The names of the selected metrics\n",
    "    \n",
    "    Returns:\n",
    "        Metricframe: The concatenation of the two MetricFrame, resricted to the metrics specified.\n",
    "    \n",
    "    \"\"\"\n",
    "    return pd.concat(\n",
    "        [mframe_1.by_group[metrics],\n",
    "        mframe_2.by_group[metrics]],\n",
    "        keys=names,\n",
    "        axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_acc_postprocess = balanced_accuracy_score(y_test, postprocess_pred)\n",
    "eq_odds_postprocess = equalized_odds_difference(y_test, postprocess_pred, sensitive_features=A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricframe_postprocess = MetricFrame(\n",
    "    metrics=fairness_metrics,\n",
    "    y_true=y_test,\n",
    "    y_pred=postprocess_pred,\n",
    "    sensitive_features=A_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricframe_postprocess.overall[metrics_to_report]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricframe_postprocess.difference()[metrics_to_report]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull this out for the discussion sections (both the performance drop and access to sensitive features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_metricframe_results(metricframe_unmitigated, metricframe_postprocess, metrics=metrics_to_report, names=[\"Unmitigated\", \"PostProcess\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricframe_postprocess.by_group[metrics_to_report].plot.bar(subplots=True, layout=[3, 1], figsize=[12,18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute the our *parity metrics* for our `ThresholdOptimizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the `ThresholdOptimizer` algorithm achieves a much lower disparity between "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reductions Approach to unfairness mitigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we took a fairness-unaware model and used the `ThresholdOptimizer` to tranform the model's decision boundary to satisfy our fairness constraints. One key limitation of the `ThresholdOptimizer` is needing access to our *sensitive_feature* during prediction time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use the *reductions* approach from [Agarwal et. al(2018)](https://arxiv.org/pdf/1803.02453.pdf) to produce models that satisify the fairness constraint without needing access to the sensitive features at deployment time.\n",
    "\n",
    "The main reduction algorithm in Fairlearn is `ExponentiatedGradient`. The algorithm creates a sequence of re-weighted datasets and retrains the wrapped classifier on each of the datasets. This re-training process is guaranteed to find a model that satisfies the fairness constraints while optimizing the performance metric.\n",
    "\n",
    "The model returned by `ExponentiatedGradient` consists of several inner models, returned by a wrapped estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instantiate an `ExponentiatedGradient` model, we pass in two parameters:\n",
    "- a base `estiamtor` (object that supports training)\n",
    "- fairness `constraints` (object of type `fairlearn.reductions.Moment`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When passing in a fairness *constaint* as a `Moment`, we can specify an `epsilon` value representing the maximum allowed difference or ratio between our largest and smallest value. For example, in the below code, `EqualizedOdds(difference_bound=epislon)` means that we are using `EqualizedOdds` as our fairness constraint, and we will allow a maximal difference of `epsilon` between our largest and smallest *Equalized Odds* value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expgrad_models_per_epsilon(estimator, epsilon, X_train, y_train, A_train):\n",
    "    \"\"\"\n",
    "    Instantiate and trains an ExponentiatedGradient model on the\n",
    "    balanced training dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        - Estimator: Base estimator to contains a `fit` and `predict` function.\n",
    "        - Epsilon: Float representing maximum difference bound for the fairness Moment constraint\n",
    "    \n",
    "    Returns:\n",
    "        - Predictors: List of inner model predictors learned by the ExponentiatedModel\n",
    "        during the training process.\n",
    "    \n",
    "    \"\"\"\n",
    "    exp_grad_est = ExponentiatedGradient(\n",
    "        estimator=estimator.named_steps[\"classifier\"],\n",
    "        constraints=EqualizedOdds(difference_bound=epsilon)\n",
    "    )\n",
    "    #Is this an issue - Re-runs\n",
    "    exp_grad_est.fit(X_train, y_train, sensitive_features=A_train)\n",
    "    predictors = exp_grad_est.predictors_\n",
    "    return predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the *performance-fairness trade-off* learned by the `ExponentiatedGradient` model is sensitive to our chose `epsilon` value, we can treat `epsilon` as a *hyperparameter* and iterate over a range of potential values. Here, we will train two `ExponentiatedGradient` models, one with `epsilon=0.01` and the second with `epsilon=0.02`, and store the inner models learned through each of the training processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0.01, 0.02]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = {}\n",
    "for eps in epsilons:\n",
    "    all_models[eps] = get_expgrad_models_per_epsilon(estimator=estimator, epsilon=eps,\n",
    "                                                    X_train=X_train, y_train=y_train, A_train=A_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epsilon, models in all_models.items():\n",
    "    print(f\"For epsilon {epsilon}, ExponentiatedGradient learned {len(models)} inner models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see all the inner models learned for each value of `epsilon`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `ExponentiatedGradient` model, we specify an `epsilon` parameter that represents the maximal disparity in our fairness metric that we will tolerate in the training process. For example, an `epsilon=0.02` means the model tolerates a maximal *equalized odds difference* of `0.2` during the training iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select best inner model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many situations due to regulation or other technical restrictions, the randomized nature of `ExponentiatedGradient` algorithm may be undesirable. In addition, the multiple inner models of the algorithm introduces challenges for model interpretability. One potential workaround to avoiding these issues is selecting one of the inner models and deploying it instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we trained multiple `ExponentiatedGradient` models at different `epsilon` levels and collected all the inner models learned by this process. However, some of these inner models are monotonically better performance in both their *balanced error rate* and *equalized odds difference*. We will filter out these *\"dominated\"* models and plot the performance trade-offs of the remaining models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pareto_efficient(points):\n",
    "    \"\"\"\n",
    "    Filters a NumPy Matrix to remove rows that are strictly dominated\n",
    "    by another row in the matrix. Stricly dominated means the all the row values are greater than\n",
    "    the values of another row.\n",
    "    \n",
    "    Parameters:\n",
    "        Points: NumPy array (NxM) of model metrics. Assumption that smaller values for metrics are preferred.\n",
    "    \n",
    "    Returns:\n",
    "        Boolean Array: Nx1 boolean mask representing the non-dominated indices.\n",
    "    \"\"\"\n",
    "    n, m = points.shape\n",
    "    is_efficient = np.ones(n, dtype = bool)\n",
    "    for i, c in enumerate(points):\n",
    "        if is_efficient[i]:\n",
    "            is_efficient[is_efficient] = np.any(points[is_efficient] < c, axis=1)\n",
    "            is_efficient[i] = True\n",
    "    return is_efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dominated_rows(points):\n",
    "    \"\"\"\n",
    "    Removes rows from a DataFrame that are monotonically dominated by another\n",
    "    row in the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        Points: DataFrame where each row represents the summarized performance (balanced accuracy, fairness metric)\n",
    "        of an inner model.\n",
    "    \n",
    "    Returns:\n",
    "        Pareto_mask: Boolean mask representing indices of input DataFrame that are not monotonically dominated.\n",
    "        Masked_Dataframe: DataFrame with dominated rows filtered out.\n",
    "    \"\"\"\n",
    "    pareto_mask = is_pareto_efficient(points.to_numpy())\n",
    "    return pareto_mask, points.loc[pareto_mask, :]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_predictor_performances(predictors, metric, X_test, Y_test, A_test=None):\n",
    "    \"\"\"\n",
    "    Helper function to compute the specified metric for all classifiers in `predictors`.\n",
    "    If no sensitive features are present, the metric is computed without disaggregation.\n",
    "    \n",
    "    Parameters:\n",
    "    - predictors: A set of classifiers to generate predictions from.\n",
    "    - metric: The metric (callable) to compute for each classifier in `predictor`\n",
    "    - X_test: The data features of the testing data set\n",
    "    - Y_test: The target labels of the teting data set\n",
    "    - A_test: The sensitive feature of the testing data set.\n",
    "    \n",
    "    Returns:\n",
    "        - List of performance scores for each classifier in `predictors`, for the given `metric`.\n",
    "    \"\"\"\n",
    "    all_predictions = [predictor.predict(X_test) for predictor in predictors]\n",
    "    if A_test is not None:\n",
    "        return [metric(Y_test, Y_sweep, sensitive_features=A_test) for Y_sweep in all_predictions]\n",
    "    else:\n",
    "        return [metric(Y_test, Y_sweep) for Y_sweep in all_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performance_sweep(models_dict, X_test, y_test, A_test):\n",
    "    \"\"\"\n",
    "    Computes the equalized_odds_difference and balanced_error_rate for a given list of inner models\n",
    "    learned by the ExponentiatedGradient algorithm. Returns a DataFrame containing the epsilon level of the model,\n",
    "    the index of the model, the equalized_odds_difference score and the balanced_error for the model.\n",
    "    \n",
    "    Parameters:\n",
    "    - models_dict: Dictionary mapping model ids to a model.\n",
    "    - X_test: The data features of the testing data set\n",
    "    - y_test: The target labels of the teting data set\n",
    "    - A_test: The sensitive feature of the testing data set.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame where each row represents a model (epsilon, index) and its performance metrics\n",
    "    \"\"\"\n",
    "    performances = []\n",
    "    for (eps, models) in models_dict.items():\n",
    "        eq_odds_difference = aggregate_predictor_performances(models, equalized_odds_difference, X_test, y_test, A_test)\n",
    "        bal_acc_score = aggregate_predictor_performances(models, balanced_accuracy_score, X_test, y_test)\n",
    "        for (i, score) in enumerate(eq_odds_difference):\n",
    "            performances.append((\n",
    "                eps,\n",
    "                i,\n",
    "                score,\n",
    "                (1-bal_acc_score[i])\n",
    "            ))\n",
    "    performances_df = pd.DataFrame.from_records(performances,\n",
    "                                               columns=[\"epsilon\", \"index\", \"equalized_odds\", \"balanced_error\"])    \n",
    "    return performances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df = model_performance_sweep(all_models, X_test, y_test, A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_subset = performance_df.loc[:, [\"equalized_odds\", \"balanced_error\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask, pareto_subset = filter_dominated_rows(performance_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df_masked = performance_df.loc[mask, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the performance trade-offs between all of our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in performance_df_masked.iterrows():\n",
    "    bal_error, eq_odds_diff = row[\"balanced_error\"], row[\"equalized_odds\"]\n",
    "    epsilon_, index_ = row[\"epsilon\"], row[\"index\"]\n",
    "    plt.scatter(bal_error, eq_odds_diff, color=\"green\", label=f\"ExponentiatedGradient\")\n",
    "    plt.text(bal_error+0.001, eq_odds_diff+0.0001, f\"Eps: {epsilon_},{index_}\", fontsize=10)\n",
    "plt.scatter(1.0 - balanced_accuracy_unmitigated, equalized_odds_unmitigated,label=f'UnmitigatedModel')\n",
    "plt.scatter(1.0 - bal_acc_postprocess, eq_odds_postprocess, label=f\"PostProcess\")\n",
    "#plt.scatter(1.0 - bal_acc_inprocess, eq_odds_inprocess, label=f\"InProcess\")\n",
    "plt.xlabel(\"Weighted Error Rate\")\n",
    "plt.ylabel(\"Equalized Odds\")\n",
    "plt.legend(bbox_to_anchor=(1.85,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above plot, we can see how the performance of the non-dominated inner models compares to the original unmitigated model. In many cases, we see that a reduction in the *equalized_odds_difference* is accompanied by a small increase in the *weighted error rate*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting an inner model of ExponentiatedGradient Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One strategy we can use to select a model is creating a *threshold* based on the *balanced error rate* of the unmitigated model. Then out of the filtered models, we select the model that minimizes the *equalized_odds_difference*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disparity is reduced, but overall performance of model is reduced. What is the correct trade-off?\n",
    "Amongst all models whose `balanced_error` are below some *threshold*, which is the best model.\n",
    "- 1.) Create threshold based on `balanced_error` of the unmitigated model.\n",
    "- 2.) Filter only models whose `balanced_error` are below the threshold.\n",
    "- 3.) Choose the model with smallest `equalized_odds` difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the context of fair lending in the United States, if a financial institution is found to be possibly engaging in discriminatory behavior, they must produce documentation that demonstrates the model chosen is the least discriminatory model while satisfying profitability and other business needs. In our approach, the business need of profitability is simulated by thresholding based on the `balanced_error` rate of the unmitigated model, and we choose the least discriminatory model based on the smallest `equalized_odds_difference` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_models_by_unmitigiated_score(all_models, models_frames, unmitigated_score,\n",
    "                                        performance_metric=\"balanced_error\", fairness_metric=\"equalized_odds\", threshold=0.01):\n",
    "    \"\"\"\n",
    "    Filters out models whose performance score is above the desired threshold.\n",
    "    Out of the remaining model, return the models with the best score on the fairness metric\n",
    "    \n",
    "    Parameters:\n",
    "    - all_models: Dictionary (Epsilon, Index) mapping (epilson, index number) pairs to a Model object\n",
    "    - models_frames: A DataFrame representing each model's performance and fairness score.\n",
    "    - unmitigated_score: The performance score of the unmitigated model.\n",
    "    - performance_metric: The model performance metric to threshold on.\n",
    "    - fairness_metric: The fairness metric to optimize for\n",
    "    - threshold: The threshold padding added to the `unmitigated_score`.\n",
    "    \"\"\"\n",
    "    models_filtered = models_frames.query(f\"{performance_metric} <= {unmitigated_score + threshold}\")\n",
    "    best_row = models_filtered.sort_values(by=[fairness_metric]).iloc[0]\n",
    "    epsilon, index = best_row[[\"epsilon\", \"index\"]]\n",
    "    return {\n",
    "        \"model\": all_models[epsilon][index],\n",
    "        \"epsilon\": epsilon,\n",
    "        \"index\": index\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = filter_models_by_unmitigiated_score(all_models,\n",
    "                                               models_frames=performance_df,\n",
    "                                               unmitigated_score=(1.0 - balanced_accuracy_unmitigated),\n",
    "                                               threshold=0.015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Epsilon for best model: {best_model.get('epsilon')}, Index number: {best_model.get('index')}\")\n",
    "inprocess_model = best_model.get(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have selected our best inner model, let's collect the model's predictions on the test dataset and compute the relevant performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_inprocess = inprocess_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_acc_inprocess = balanced_accuracy_score(y_test, y_pred_inprocess)\n",
    "eq_odds_inprocess = equalized_odds_difference(y_test, y_pred_inprocess, sensitive_features=A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricframe_inprocess = MetricFrame(\n",
    "    metrics=fairness_metrics,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_inprocess,\n",
    "    sensitive_features=A_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricframe_inprocess.difference()[metrics_to_report]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricframe_inprocess.overall[metrics_to_report]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricframe_inprocess.by_group[metrics_to_report].plot.bar(subplots=True, layout=[3, 1], figsize=[12, 18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss Performance and Trade-Offs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have trained two different fairness-aware models using the *post-processing* approach and the *reductions* approach. Let's compare the performance of these models to our original fairness-unaware model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_error_pairs = [\n",
    "    (\"balanced_accuracy\", \"balanced_acc_error\"),\n",
    "    (\"false_positive_rate\", \"false_positive_error\"),\n",
    "    (\"false_negative_rate\", \"false_negative_error\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metricframe_w_errors(mframe, metrics_to_report, metric_error_pair):\n",
    "    mframe_by_group = mframe.by_group.copy()\n",
    "    for (metric_name, error_name) in metric_error_pair:\n",
    "        mframe_by_group[metric_name] = mframe_by_group[metric_name].apply(lambda x: f'{x:.3f}')\n",
    "        mframe_by_group[error_name] = mframe_by_group[error_name].apply(lambda x: f'{x:.3f}')\n",
    "        mframe_by_group[metric_name] = mframe_by_group[metric_name].str.cat(mframe_by_group[error_name], sep=\"±\")\n",
    "    return mframe_by_group[metrics_to_report]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report model performance error bars for metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_metricframe_w_errors(metricframe_unmitigated, metrics_to_report, metric_error_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricframe_unmitigated.overall[metrics_to_report]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_metricframe_w_errors(metricframe_inprocess, metrics_to_report, metric_error_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricframe_inprocess.overall[metrics_to_report]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_metricframe_w_errors(metricframe_postprocess, metrics_to_report, metric_error_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricframe_postprocess.overall[metrics_to_report]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see both of our fairness-aware models yield a slight decrease in the *balanced_accuracy* for *male applicants* compared to our fairness-unaware model. In the *reductions* model, we see a decrease in the *false positive rate* for *female applicants*. This is accompanied by an increase in the *false negative rate* for *male applicants*. However overall, the *equalized odds difference* for the *reductions* models is lower than that of the original fairness-unaware model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case study, we walked through the process of assessing a credit decision model for gender-related performance disparities. Our analysis follows closely from the work done in [*Dudik et al.* (2020)](https://www.microsoft.com/en-us/research/uploads/prod/2020/09/Fairlearn-EY_WhitePaper-2020-09-22.pdf) where we used the *Fairlearn* toolkit to perform an audit of a fairness-unaware tree-based model. We applied a *post-processing* and *reductions* mitigation technique to mitigate the *equalized odds difference* in our model.\n",
    "\n",
    "Through the *reductions* process, we generated a model that reduces the *equalized odds difference* of the original model without a drastic increase in the *balanced error score*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing a Model Card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key facet of Responsible Machine Learning is responsible documentation practices. *Mitchell et al. (2019)* proposed the model card framework for documentating and reporting model training details and deployment considerations. \n",
    "\n",
    "A *model card* contains sections for documenting training and evaluation dataset descriptions, ethical concerns, and quantitative evaluation summaries. In practice, we would ideally create a model card for our model before deploying it in production. Although we will not be producing a model card in this case study, interested readers can learn more about creating model cards using the *Model Card Toolkit* from [the Fairlearn SciPy 2021 tutorial](https://colab.research.google.com/github/fairlearn/talks/blob/main/2021_scipy_tutorial/fairness-in-AI-systems-student.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness is not the same as anti-discrimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although they are intertwined concepts, algorithmic fairness is not the same concept as anti-dscrimination law. An AI system can comply with anti-discrimination law while exhibiting fairness-related concerns. On the other hand, some fairness interventions may be illegal under anti-discrimination law. In [Xiang and Raji (2019)](https://arxiv.org/abs/1912.00761), the authors discuss the compatabilities and disconnects between anti-discrimination law and algorithmic notions of fairness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fairness Under Unawareness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When proving credit models are compliant with fair lending laws, practitoners may run into the issue of not having access to the sensitive demographic features. As a result, financial institutions are often tasked with proving their models are compliant with fair lending laws without having access to the sensitive feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, under the *Equal Opportunity Credit Act*, financial lenders in the United States must audit credit decision models for racial disparity without having access to an applicant's *race*. In these setting, financial institutions impute an applicant's *race* using *Bayesian Informed Surname Geoencoding* (BISG); based on the applicant's state of resident and surname. In [Chen et. al (2018)](https://arxiv.org/abs/1811.11154), the authors analyze the *BISG* framework and demonstrate the method systemically over-estimates the true level of disparate impact."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
